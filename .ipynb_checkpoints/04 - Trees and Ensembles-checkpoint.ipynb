{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Trees and Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>html, body{overflow-y: visible !important} .CodeMirror{min-width:105% !important;} .rise-enabled .CodeMirror, .rise-enabled .output_subarea{font-size:140%; line-height:1.2; overflow: visible;} .output_subarea pre{width:110%}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Global imports and settings\n",
    "from preamble import *\n",
    "%matplotlib inline\n",
    "plt.rcParams['savefig.dpi'] = 120 # Use 300 for PDF, 100 for slides\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"\n",
    "HTML('''<style>html, body{overflow-y: visible !important} .CodeMirror{min-width:105% !important;} .rise-enabled .CodeMirror, .rise-enabled .output_subarea{font-size:140%; line-height:1.2; overflow: visible;} .output_subarea pre{width:110%}</style>''') # For slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-89e69ba569a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmglearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplots\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_animal_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Dropbox\\University\\ML-course\\mglearn\\plot_animal_tree.py\u001b[0m in \u001b[0;36mplot_animal_tree\u001b[1;34m(ax)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot_animal_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mimport\u001b[0m \u001b[0mgraphviz\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0max\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgca\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'"
     ]
    }
   ],
   "source": [
    "mglearn.plots.plot_animal_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Building Decision Trees\n",
    "* Split the data in two (or more) parts\n",
    "* Search over all possible splits and choose the one that is most _informative_\n",
    "    * Many heuristics\n",
    "    * E.g. _information gain_: how much does the entropy of the class labels decrease after the split (purer 'leafs')\n",
    "* Repeat recursive partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions:\n",
    "    \n",
    "* Classification: find leaf for new data point, predict majority class (or class distribution)\n",
    "* Regression: idem, but predict the _mean_ of all values    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Decision Tree classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "mglearn.plots.plot_tree_progressive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Heuristics\n",
    "\n",
    "For classification ($X_i \\rightarrow {class}_{k}$): *Impurity measures*:\n",
    "\n",
    "- Misclassification Error (leads to larger trees):\n",
    "$$ 1 - \\underset{k}{\\operatorname{argmax}} \\hat{p}_{k} $$\n",
    "\n",
    "- Gini-Index (probabilistic predictions):\n",
    "$$ \\sum_{k\\neq k'} \\hat{p}_k \\hat{p}_{k'} = \\sum_{k=1}^K \\hat{p}_k(1-\\hat{p}_k) $$\n",
    "\n",
    "with $\\hat{p}_k$ = the relative frequency of class $k$ in the leaf node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Entropy (of the class attribute) measures *unpredictability* of the data:\n",
    "    - How likely will random example have class k?\n",
    "$$ E(X) = -\\sum_{k=1}^K \\hat{p}_k \\log_{2}\\hat{p}_k $$\n",
    "\n",
    "- Information Gain (a.k.a. Kullbackâ€“Leibler divergence) for choosing attribute $X_i$ to split the data:\n",
    "$$ G(X,X_i) = E(X) - \\sum_{v=1}^V \\frac{|X_{i=v}|}{|X_{i}|} E(X_{i=v}) $$\n",
    "\n",
    "with $\\hat{p}_k$ = the relative frequency of class $k$ in the leaf node,  $X$ = the training set, containing $i$ features (variables) $X_i$, $v$ a specific value for $X_i$, $X_{i=v}$ is the set of examples having value $v$ for feature $X_i$: $\\{x \\in X | X_i = v\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Heuristics visualized (binary class)\n",
    "- Note that `gini != entropy/2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def gini(p):\n",
    "   return (p)*(1 - (p)) + (1 - p)*(1 - (1-p))\n",
    "\n",
    "def entropy(p):\n",
    "   return - p*np.log2(p) - (1 - p)*np.log2((1 - p))\n",
    "\n",
    "def classification_error(p):\n",
    "   return 1 - np.max([p, 1 - p])\n",
    "\n",
    "x = np.arange(0.0, 1.0, 0.01)\n",
    "ent = [entropy(p) if p != 0 else None for p in x]\n",
    "scaled_ent = [e*0.5 if e else None for e in ent]\n",
    "c_err = [classification_error(i) for i in x]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "for j, lab, ls, c, in zip(\n",
    "      [ent, scaled_ent, gini(x), c_err],\n",
    "      ['Entropy', 'Entropy (scaled)', 'Gini Impurity', 'Misclassification Error'],\n",
    "      ['-', '-', '--', '-.'],\n",
    "      ['lightgray', 'red', 'green', 'blue']):\n",
    "   line = ax.plot(x, j, label=lab, linestyle=ls, lw=1, color=c)\n",
    "\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(0.01, 0.85),\n",
    "         ncol=1, fancybox=True, shadow=False)\n",
    "ax.axhline(y=0.5, linewidth=1, color='k', linestyle='--')\n",
    "ax.axhline(y=1.0, linewidth=1, color='k', linestyle='--')\n",
    "\n",
    "plt.ylim([0, 1.1])\n",
    "plt.xlabel('p(j=1)')\n",
    "plt.ylabel('Impurity Index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "Ex.| 1 | 2 | 3 | 4 | 5 | 6\n",
    "---|---|---|---|---|---|---\n",
    "a1 | T | T | T | F | F | F\n",
    "a2 | T | T | F | F | T | T\n",
    "class | + | + | - | + | - | -\n",
    "\n",
    "$E(X)$ ?  \n",
    "$G(X, X_{a2})$ ?  \n",
    "$G(X, X_{a1})$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$E(X)$ = $-(\\frac{1}{2}*log_2(\\frac{1}{2})+\\frac{1}{2}*log_2(\\frac{1}{2})) = 1$ (classes have equal probabilities)    \n",
    "$G(X, X_{a2})$ = 0 (after split, classes still have equal probabilities, entropy stays 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ex.| 1 | 2 | 3 | 4 | 5 | 6\n",
    "---|---|---|---|---|---|---\n",
    "a1 | T | T | T | F | F | F\n",
    "a2 | T | T | F | F | T | T\n",
    "class | + | + | - | + | - | -\n",
    "\n",
    "$$ E(X) = -\\sum_{k=1}^K \\hat{p}_k \\log\\hat{p}_k \\quad , \\quad G(X,X_i) = E(X) - \\sum_{v=1}^V \\frac{|X_{i=v}|}{|X_{i}|} E(X_{i=v}) $$\n",
    "\n",
    "$$E(X_{a1=T}) = - \\frac{2}{3} \\log_{2}(\\frac{2}{3}) - \\frac{1}{3} \\log_{2}(\\frac{1}{3}) = 0.9183 \\quad (= E(X_{a1=F}))$$\n",
    "$$G(X, X_{a1}) = 1 - \\frac{1}{2} 0.9183 - \\frac{1}{2} 0.9183 = 0.0817 $$\n",
    "\n",
    "hence we split on a1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Heuristics in scikit-learn\n",
    "\n",
    "The splitting criterion can be set with the `criterion` option in `DecisionTreeClassifier`\n",
    "\n",
    "- `gini` (default): gini impurity index\n",
    "- `entropy`: information gain\n",
    "\n",
    "Best value depends on dataset, as well as other hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import plot_classifiers as pc\n",
    "names = [\"Decision tree - gini\", \"Decision tree - entropy\"]\n",
    "\n",
    "classifiers = [\n",
    "    DecisionTreeClassifier(),\n",
    "    DecisionTreeClassifier(criterion=\"entropy\")\n",
    "    ]\n",
    "\n",
    "pc.plot_classifiers(names, classifiers, figuresize=(20,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Handling many-valued features\n",
    "What happens when a feature has (almost) as many values as examples?\n",
    "- Information Gain will select it\n",
    "\n",
    "One approach: use Gain Ratio instead (not available scikit-learn):\n",
    "$$ GainRatio(X,X_i) = \\frac{Gain(X,X_i)}{SplitInfo(X,X_i)}$$  \n",
    "$$ SplitInfo(X,X_i) = - \\sum_{v=1}^V \\frac{|X_{i=v}|}{|X|} log_{2} \\frac{|X_{i=v}|}{|X|} $$\n",
    "\n",
    "where $X_{i=v}$ is the subset of examples for which feature $X_i$ has value v.\n",
    "\n",
    "SplitInfo will be big if $X_i$ fragments the data into many small subsets, resulting in a smaller Gain Ratio. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Overfitting: Controlling complexity of Decision Trees\n",
    "Decision trees can very easily overfit the data. Regularization strategies:  \n",
    "\n",
    "* Pre-pruning: stop creation of new leafs at some point\n",
    "    * Limiting the depth of the tree, or the number of leafs\n",
    "    * Requiring a minimal leaf size (number of instances)\n",
    "* Post-pruning: build full tree, then prune (join) leafs\n",
    "    * Reduced error pruning: evaluate against held-out data\n",
    "    * Many other strategies exist. \n",
    "    * scikit-learn supports none of them (yet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Effect of pre-pruning: default tree overfits, setting `max_depth=4` is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "6e5d7a76-9bba-42f7-b26e-907775d289b2"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, stratify=cancer.target, random_state=42)\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=5, random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Analyzing Decision Trees manually\n",
    "\n",
    "- Visualize and find the path that most data takes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a .dot file\n",
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(tree, out_file=\"tree.dot\", class_names=[\"malignant\", \"benign\"], \n",
    "                feature_names=cancer.feature_names, impurity=False, filled=True)\n",
    "# Open and display\n",
    "import graphviz\n",
    "with open(\"tree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "display(graphviz.Source(dot_graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`DecisionTreeClassifier` also returns _feature importances_\n",
    "\n",
    "* In [0,1], sum up to 1\n",
    "* High values for features selected by the algorithm\n",
    "* Other features may also be relevant, but don't contribute new information given the selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "dc2f68ee-0df0-47ed-b500-7ec99d5a0a5d"
   },
   "outputs": [],
   "source": [
    "# Feature importances sum up to 1\n",
    "print(\"Feature importances:\\n{}\".format(tree.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_feature_importances_cancer(model):\n",
    "    n_features = cancer.data.shape[1]\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(np.arange(n_features), cancer.feature_names)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.ylim(-1, n_features)\n",
    "\n",
    "plt.rcParams.update({'font.size': 8})\n",
    "plot_feature_importances_cancer(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree regression\n",
    "Heuristic for regression ($x_i \\rightarrow y_i \\in {\\mathrm I\\!R}$): *Minimal quadratic distance*\n",
    "\n",
    "- Consider splits at every data point for every variable (or halfway between data points)\n",
    "- Dividing the data on split variable $X_j$ at splitpoint $s$ leads to the following half-spaces:\n",
    "\n",
    "$$ R_1(j, s) = { X : X_j \\leq s} \\quad and \\quad R_2(j, s) = { X : X_j > s} $$\n",
    "    \n",
    "- The best split variable and the corresponding splitpoint, with predicted value $c_i$ and actual value $Y_i$:\n",
    "\n",
    "$$ \\min_{j,s} \\left(\\min_{c_1} \\sum_{x_{i} \\in R_1(j,s)} (y_i - c_1)^2 + \\min_{c_2} \\sum_{x_{i} \\in R_2(j,s)} (y_i - c_2)^2 \\right) $$\n",
    "\n",
    "- Assuming that the tree predicts $y_i$ as the average of all $x_i$ in the leaf:\n",
    "    \n",
    "$$ \\hat{c}_1 = \\text{avg}(y_i | x_{i} \\in R_1(j,s)) \\quad and \\quad \\hat{c}_2 = \\text{avg}(y_i | x_{i} \\in R_2(j,s)) $$\n",
    "\n",
    "with $x_i$ being the i-th example in the data, with target value $y_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### In scikit-learn\n",
    "Regression is done with  `DecisionTreeRegressor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_decision_tree_regression(regr_1, regr_2):\n",
    "    # Create a random dataset\n",
    "    rng = np.random.RandomState(1)\n",
    "    X = np.sort(5 * rng.rand(80, 1), axis=0)\n",
    "    y = np.sin(X).ravel()\n",
    "    y[::5] += 3 * (0.5 - rng.rand(16))\n",
    "\n",
    "    # Fit regression model\n",
    "    regr_1.fit(X, y)\n",
    "    regr_2.fit(X, y)\n",
    "\n",
    "    # Predict\n",
    "    X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n",
    "    y_1 = regr_1.predict(X_test)\n",
    "    y_2 = regr_2.predict(X_test)\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(X, y, c=\"darkorange\", label=\"data\")\n",
    "    plt.plot(X_test, y_1, color=\"cornflowerblue\", label=\"max_depth=2\", linewidth=2)\n",
    "    plt.plot(X_test, y_2, color=\"yellowgreen\", label=\"max_depth=5\", linewidth=2)\n",
    "    plt.xlabel(\"data\")\n",
    "    plt.ylabel(\"target\")\n",
    "    plt.title(\"Decision Tree Regression\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "regr_1 = DecisionTreeRegressor(max_depth=2)\n",
    "regr_2 = DecisionTreeRegressor(max_depth=5)\n",
    "\n",
    "plot_decision_tree_regression(regr_1,regr_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that decision trees do not extrapolate well. \n",
    "\n",
    "- The leafs return the same _mean_ value no matter how far the new data point lies from the training examples.\n",
    "- Example on the `ram_price` forecasting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "ram_prices = pd.read_csv('data/ram_price.csv')\n",
    "\n",
    "plt.semilogy(ram_prices.date, ram_prices.price)\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Price in $/Mbyte\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Use historical data to forecast prices after the year 2000\n",
    "data_train = ram_prices[ram_prices.date < 2000]\n",
    "data_test = ram_prices[ram_prices.date >= 2000]\n",
    "\n",
    "# predict prices based on date:\n",
    "X_train = data_train.date[:, np.newaxis]\n",
    "# we use a log-transform to get a simpler relationship of data to target\n",
    "y_train = np.log(data_train.price)\n",
    "\n",
    "tree = DecisionTreeRegressor().fit(X_train, y_train)\n",
    "linear_reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "# predict on all data\n",
    "X_all = ram_prices.date[:, np.newaxis]\n",
    "\n",
    "pred_tree = tree.predict(X_all)\n",
    "pred_lr = linear_reg.predict(X_all)\n",
    "\n",
    "# undo log-transform\n",
    "price_tree = np.exp(pred_tree)\n",
    "price_lr = np.exp(pred_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['lines.linewidth'] = 2\n",
    "plt.semilogy(data_train.date, data_train.price, label=\"Training data\")\n",
    "plt.semilogy(data_test.date, data_test.price, label=\"Test data\")\n",
    "plt.semilogy(ram_prices.date, price_tree, label=\"Tree prediction\")\n",
    "plt.semilogy(ram_prices.date, price_lr, label=\"Linear prediction\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Strengths, weaknesses and parameters\n",
    "Pre-pruning: regularize by:\n",
    "\n",
    "* Setting a low `max_depth`, `max_leaf_nodes`\n",
    "* Setting a higher `min_samples_leaf` (default=1)\n",
    "\n",
    "Decision trees: \n",
    "\n",
    "* Work well with features on completely different scales, or a mix of binary and continuous features\n",
    "    * Does not require normalization\n",
    "* Interpretable, easily visualized\n",
    "* Still tend to overfit easily. Use ensembles of trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ensemble learning\n",
    "Ensembles are methods that combine multiple machine learning models to create more powerful models. Most popular are:\n",
    "\n",
    "* __RandomForests__: Build randomized trees on random samples of the data\n",
    "* __Gradient boosting machines__: Build trees iteratively, giving higher weights to the points misclassified by previous trees\n",
    "\n",
    "In both cases, predictions are made by doing a vote over the members of the example.  \n",
    "__Stacking__ is another technique that builds a (meta)model over the predictions of each member."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### RandomForests\n",
    "Reduce overfitting by averaging out individual predictions (variance reduction)\n",
    "\n",
    "* Take a _bootstrap sample_ of your data\n",
    "    * Randomly sample with replacement\n",
    "    * Build a tree on each bootstrap\n",
    "* Repeat `n_estimators` times \n",
    "    * Higher values: more trees, more smoothing\n",
    "    * Make prediction by aggrating the individual tree predictions\n",
    "        * a.k.a. Bootstrap aggregating (Bagging)\n",
    "* RandomForest: Randomize trees by considering only a random subset of features of size `max_features` _in each node_\n",
    "    * Small `max_features` yields more different trees, more smoothing\n",
    "    * Default: $sqrt(n\\_features)$ for classification, $log2(n\\_features)$ for regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Making predictions:\n",
    "* Classification: soft voting (softmax)\n",
    "    * Every member returns probability for each class\n",
    "    * After averaging, the class with highest probability wins\n",
    "* Regression:  \n",
    "    * Return the _mean_ of all predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "uuid": "76ce4154-b441-475e-97e3-1b507964eb29"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.25, random_state=3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
    "                                                    random_state=42)\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=10, random_state=2)\n",
    "forest.fit(X_train, y_train) \n",
    " \n",
    "plt.rcParams.update({'font.size': 12})\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 10))\n",
    "for i, (ax, tree) in enumerate(zip(axes.ravel(), forest.estimators_)):\n",
    "    ax.set_title(\"Tree {}\".format(i))\n",
    "    mglearn.plots.plot_tree_partition(X_train, y_train, tree, ax=ax)\n",
    "    \n",
    "mglearn.plots.plot_2d_separator(forest, X_train, fill=True, ax=axes[-1, -1],\n",
    "                                alpha=.4)\n",
    "axes[-1, -1].set_title(\"Random Forest\")\n",
    "mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Scikit-learn algorithms:\n",
    "\n",
    "- `RandomForestClassifier` (or Regressor)\n",
    "- `ExtraTreesClassifier`: Grows deeper trees, faster\n",
    "\n",
    "Most important parameters:\n",
    "\n",
    "* `n_estimators` (higher is better, but diminishing returns)\n",
    "    * Will start to underfit (bias error component increases slightly)\n",
    "* `max_features` (default is typically ok)\n",
    "    * Set smaller to reduce space/time requirements\n",
    "* parameters of trees, e.g. `max_depth` (less effect)\n",
    "\n",
    "`n_jobs` sets the number of parallel cores to run  \n",
    "`random_state` should be fixed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, random_state=0)\n",
    "forest = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=100, random_state=0) # Vary n_estimators\n",
    "forest.fit(X_train, y_train) \n",
    "print(\"Accuracy on training set: {:.3f}\".format(forest.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(forest.score(X_test, y_test))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "RandomForest allow another way to evaluate performance: out-of-bag (OOB) error\n",
    "\n",
    "- While growing forest, estimate test error from training samples\n",
    "- For each tree grown, 33-36% of samples are not selected in bootstrap\n",
    "    - Called the 'out of bootstrap' (OOB) samples\n",
    "    - Predictions are made as if they were novel test samples\n",
    "    - Through book-keeping, majority vote is computed for all OOB samples from all trees\n",
    "- OOB estimated test error is rather accurate in practice\n",
    "    - As good as CV estimates, but can be computed on the fly (without repeated model fitting)\n",
    "    - Tends to be slightly pessimistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In scikit-learn OOB error are returned as follows:\n",
    "```\n",
    "oob_error = 1 - clf.oob_score_\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "RANDOM_STATE = 123\n",
    "\n",
    "# Generate a binary classification dataset.\n",
    "X, y = make_classification(n_samples=500, n_features=25,\n",
    "                           n_clusters_per_class=1, n_informative=15,\n",
    "                           random_state=RANDOM_STATE)\n",
    "\n",
    "# NOTE: Setting the `warm_start` construction parameter to `True` disables\n",
    "# support for parallelized ensembles but is necessary for tracking the OOB\n",
    "# error trajectory during training.\n",
    "ensemble_clfs = [\n",
    "    (\"RandomForestClassifier, max_features='sqrt'\",\n",
    "        RandomForestClassifier(warm_start=True, oob_score=True,\n",
    "                               max_features=\"sqrt\",\n",
    "                               random_state=RANDOM_STATE)),\n",
    "    (\"RandomForestClassifier, max_features='log2'\",\n",
    "        RandomForestClassifier(warm_start=True, max_features='log2',\n",
    "                               oob_score=True,\n",
    "                               random_state=RANDOM_STATE)),\n",
    "    (\"RandomForestClassifier, max_features=None\",\n",
    "        RandomForestClassifier(warm_start=True, max_features=None,\n",
    "                               oob_score=True,\n",
    "                               random_state=RANDOM_STATE))\n",
    "]\n",
    "\n",
    "# Map a classifier name to a list of (<n_estimators>, <error rate>) pairs.\n",
    "error_rate = OrderedDict((label, []) for label, _ in ensemble_clfs)\n",
    "\n",
    "# Range of `n_estimators` values to explore.\n",
    "min_estimators = 15\n",
    "max_estimators = 175\n",
    "\n",
    "for label, clf in ensemble_clfs:\n",
    "    for i in range(min_estimators, max_estimators + 1):\n",
    "        clf.set_params(n_estimators=i)\n",
    "        clf.fit(X, y)\n",
    "\n",
    "        # Record the OOB error for each `n_estimators=i` setting.\n",
    "        oob_error = 1 - clf.oob_scor e_\n",
    "        error_rate[label].append((i, oob_error))\n",
    "\n",
    "# Generate the \"OOB error rate\" vs. \"n_estimators\" plot.\n",
    "for label, clf_err in error_rate.items():\n",
    "    xs, ys = zip(*clf_err)\n",
    "    plt.plot(xs, ys, label=label)\n",
    "\n",
    "plt.xlim(min_estimators, max_estimators)\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"OOB error rate\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Feature importance\n",
    "RandomForests provide more reliable feature importances, based on many alternative hypotheses (trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size':8})\n",
    "plot_feature_importances_cancer(forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Strengths, weaknesses and parameters\n",
    "RandomForest are among most widely used algorithms:\n",
    "\n",
    "* Don't require a lot of tuning\n",
    "* Typically very accurate models\n",
    "* Handles heterogeneous features well\n",
    "* Implictly selects most relevant features\n",
    "\n",
    "Downsides:\n",
    "\n",
    "* less interpretable, slower to train (but parallellizable)\n",
    "* don't work well on high dimensional sparse data (e.g. text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Boosted Regression Trees (Gradient Boosting Machines)\n",
    "Instead of reducing the variance of overfitted models, reduce the bias of underfitted models\n",
    "\n",
    "* Use strong pre-pruning to build very shallow trees\n",
    "    * Default `max_depth`=3\n",
    "* Iteratively build new trees by increasing weights of points that were badly predicted\n",
    "* Example of _additive modelling_: each tree depends on the outcome of previous trees\n",
    "* Optimization: find optimal weights for all data points\n",
    "    * Gradient descent (covered later) finds optimal set of weights\n",
    "    * `learning rate` controls how strongly the weights are altered in each iteration (default 0.1)\n",
    "* Repeat `n_estimators` times (default 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Example:\n",
    "![boosting](images/04_boosting_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "After 1 iteration\n",
    "\n",
    "- The simple decision tree divides space\n",
    "- Misclassified points get higher weight (larger dots)\n",
    "\n",
    "![boosting](images/04_boosting_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "After 3 iterations\n",
    "\n",
    "![boosting](images/04_boosting_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "After 20 iterations\n",
    "\n",
    "![boosting](images/04_boosting_20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Each tree provides good predictions on part of the data, use voting for final prediction\n",
    "\n",
    "* Soft voting for classification, mean values for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import plot_classifiers as pc\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "names = [\"Gradient Boosting 1 tree\", \"Gradient Boosting 3 trees\", \"Gradient Boosting 100 trees\"]\n",
    "\n",
    "classifiers = [\n",
    "    GradientBoostingClassifier(n_estimators=1, random_state=0, learning_rate=0.5),\n",
    "    GradientBoostingClassifier(n_estimators=3, random_state=0, learning_rate=0.5),\n",
    "    GradientBoostingClassifier(n_estimators=100, random_state=0, learning_rate=0.5)\n",
    "    ]\n",
    "\n",
    "pc.plot_classifiers(names, classifiers, figuresize=(20,8))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Tuning \n",
    "- n_estimators: Higher is better, but will start to overfit\n",
    "- learning_rate: Lower rates mean more trees are needed to get more complex models\n",
    "    - Main regularizer, also known as 'shrinkage'\n",
    "    - Set n_estimators as high as possible, then tune learning_rate\n",
    "- max_depth: typically kept low (<5), reduce when overfitting\n",
    "- loss: Loss function used for gradient descent (defaults OK)\n",
    "    - Classification:\n",
    "        - `deviance` (default): log-likelihood loss (as in logistic regression)\n",
    "        - `exponential`: exponential loss (AdaBoost algorithm)\n",
    "    - Regression:\n",
    "        - `ls`: Least squares (typically the best option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, random_state=0)\n",
    "\n",
    "gbrt = GradientBoostingClassifier(random_state=0)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are overfitting. We can decrease max_depth\n",
    "gbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or decrease the learning rate (less effect) \n",
    "gbrt = GradientBoostingClassifier(random_state=0, learning_rate=0.01)\n",
    "gbrt.fit(X_train, y_train) \n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Gradient boosting machines use much simpler trees\n",
    "- Hence, tends to completely ignore some of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "plot_feature_importances_cancer(gbrt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Strengths, weaknesses and parameters\n",
    "* Among the most powerful and widely used models\n",
    "* Work well on heterogeneous features and different scales\n",
    "* Require careful tuning, take longer to train.\n",
    "* Does not work well on high-dimensional sparse data\n",
    "\n",
    "Main hyperparameters:\n",
    "\n",
    "* `n_estimators`: Higher is better, but will start to overfit\n",
    "* `learning_rate`: Lower rates mean more trees are needed to get more complex models\n",
    "    * Set `n_estimators` as high as possible, then tune `learning_rate`\n",
    "* `max_depth`: typically kept low (<5), reduce when overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### XGBoost\n",
    "\n",
    "XGBoost is another python library for gradient boosting (install separately). \n",
    "\n",
    "- The main difference lies the use of approximation techniques to make it faster.\n",
    "    - Hence, you can do 10x (or 100x) more boosting iterations in same amount of time\n",
    "- Sketching: Given 10000 possible splits, it will only consider 300 \"good enough\" splits by default\n",
    "    - Controlled by the `sketch_eps` parameter (default 0.03)\n",
    "- Loss function approximation with Taylor Expansion: more efficient way to evaluate splits\n",
    "\n",
    "Further reading:\n",
    "[XGBoost Documentation](https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-tree-booster)\n",
    "[Paper](http://arxiv.org/abs/1603.02754)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plot_classifiers as pc\n",
    "names = [\"Decision tree\", \"Random Forest 10\", \"Gradient Boosting\"]\n",
    "\n",
    "classifiers = [\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=100, max_features=1),\n",
    "    GradientBoostingClassifier(random_state=0, learning_rate=0.5)\n",
    "    ]\n",
    "\n",
    "pc.plot_classifiers(names, classifiers, figuresize=(20,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary\n",
    "- Bagging / RandomForest is a variance-reduction technique\n",
    "    - Build many high-variance (overfitting) models\n",
    "        - Typically deep (randomized) decision trees\n",
    "        - The more different the models, the better\n",
    "    - Aggregation (soft voting or averaging) reduces variance\n",
    "    - Parallellizes easily\n",
    "- Boosting is a bias-reduction technique\n",
    "    - Build many high-bias (underfitting) models\n",
    "        - Typically shallow decision trees\n",
    "        - Sample weights are updated to create different trees\n",
    "    - Aggregation (soft voting or averaging) reduces bias\n",
    "    - Doesn't parallelize easily\n",
    "        - Approximation techniques exist to speed up calculation\n",
    "- You can build ensembles with other models as well\n",
    "    - Especially if they show high variance or bias\n",
    "- It is also possible to build _heterogeneous_ ensembles\n",
    "    - Models from different algorithms\n",
    "    - Are combined by letting each algorithm predict\n",
    "    - Often a meta-classifier/regressor is trained on the predictions: Stacking"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_metadata": {
   "author": "Andreas C. M\\\"ller",
   "title": "Machine Learning with Python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
